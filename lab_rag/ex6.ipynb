{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd4034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    host=host,\n",
    "    port=port\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d91710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "VECTOR_LENGTH = 768 \n",
    "\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"Primary id\")\n",
    "text = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=4096, description=\"Page text\")\n",
    "embedding_text = FieldSchema(\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_LENGTH, description=\"Embedded text\")\n",
    "\n",
    "fields = [id_field, text, embedding_text]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, auto_id=True, enable_dynamic_field=True, description=\"RAG Texts collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1995c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 457750080042434562, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 457752224516800515}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\", \n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64}  # lower values for speed\n",
    ") \n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2486f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping existing collection: rag_texts_and_embeddings\n",
      "Collection dropped successfully\n"
     ]
    }
   ],
   "source": [
    "#Its just for me. I use it in the case if I restart my project and need it for run the previous command without issues.\n",
    "#It detects if the project \n",
    "\n",
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "if milvus_client.has_collection(COLLECTION_NAME):\n",
    "    print(f\"Dropping existing collection: {COLLECTION_NAME}\")\n",
    "    milvus_client.drop_collection(COLLECTION_NAME)\n",
    "    print(\"Collection dropped successfully\")\n",
    "else:\n",
    "    print(\"Collection doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f04a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data source and destination\n",
    "## the document origin destination from which document will be downloaded \n",
    "pdf_url = \"https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the document\n",
    "file_name = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the processed document \n",
    "file_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.json\"\n",
    "\n",
    "## local destination of the embedded pages of the document\n",
    "embeddings_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska-Embeddings.json\"\n",
    "\n",
    "## local destination of all above local required files\n",
    "data_dir = \"./milvus_db/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e405d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_pdf_data(pdf_url: str, file_name: str) -> None:\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as file:\n",
    "        for block in response.iter_content(chunk_size=1024):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "\n",
    "download_pdf_data(pdf_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9509b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_name, file_json):\n",
    "    document = fitz.open(os.path.join(data_dir, file_name))\n",
    "    pages = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "\n",
    "    with open(os.path.join(data_dir, file_json), \"w\") as file:\n",
    "        json.dump(pages, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "extract_pdf_text(file_name, file_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bdbad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def generate_embeddings(file_json, embeddings_json, model):\n",
    "    pages = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for page in data:\n",
    "        pages.append(page[\"text\"])\n",
    "\n",
    "    embeddings = model.encode(pages)\n",
    "\n",
    "    embeddings_paginated = []\n",
    "    for page_num in range(len(embeddings)):\n",
    "        embeddings_paginated.append({\"page_num\": page_num, \"embedding\": embeddings[page_num].tolist()})\n",
    "\n",
    "    with open(os.path.join(data_dir, embeddings_json), \"w\") as file:\n",
    "        json.dump(embeddings_paginated, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "model_name = \"ipipan/silver-retriever-base-v1.1\"\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Error from my console: (I dont know why it does not work with WSL now. I have to make a rsearch. It did work on windows earlier with the same GPU though)\n",
    "# NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
    "# The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
    "device = \"cpu\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "generate_embeddings(file_json, embeddings_json, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9764325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings(file_json, embeddings_json, client=milvus_client):\n",
    "    rows = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as t_f, open(os.path.join(data_dir, embeddings_json), \"r\") as e_f:\n",
    "        text_data, embedding_data = json.load(t_f), json.load(e_f)\n",
    "        text_data =  list(map(lambda d: d[\"text\"], text_data))\n",
    "        embedding_data = list(map(lambda d: d[\"embedding\"], embedding_data))\n",
    "        \n",
    "        for page, (text, embedding) in enumerate(zip(text_data, embedding_data)):\n",
    "            rows.append({\"text\":text, \"embedding\": embedding})\n",
    "\n",
    "    client.insert(collection_name=\"rag_texts_and_embeddings\", data=rows)\n",
    "\n",
    "\n",
    "insert_embeddings(file_json, embeddings_json)\n",
    "\n",
    "# load inserted data into memory\n",
    "milvus_client.load_collection(\"rag_texts_and_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623ef1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(model, query, client=milvus_client):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    result = client.search(\n",
    "        collection_name=\"rag_texts_and_embeddings\", \n",
    "        data=[embedded_query], \n",
    "        limit=1,\n",
    "        search_params={\"metric_type\": \"L2\"},\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "#result = search(model, query=\"Czym jest sztuczna inteligencja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84e90eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_KEY)\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "gemini_model = genai.GenerativeModel(MODEL)\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    try:\n",
    "        # Send request to Gemini 2.0 Flash API and get the response\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c80232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_prompt(context: str, question: str) -> str:\n",
    "    return f\"\"\"Na podstawie poniższych fragmentów dokumentu odpowiedz na pytanie.\n",
    "    \n",
    "    Kontekst:\n",
    "    {context}\n",
    "    \n",
    "    Pytanie: {question}\n",
    "    Odpowiedź:\"\"\"\n",
    "\n",
    "def rag(query: str):\n",
    "    # Retrieve context\n",
    "    results = search(model, query=query)\n",
    "    context = \"\\n\".join(hit[\"entity\"][\"text\"] for hit in results[0])\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_response(build_prompt(context, query))\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e01279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sztuczna inteligencja (AI) to obszar informatyki, który skupia się na tworzeniu programów komputerowych zdolnych do wykonywania zadań, które wymagają ludzkiej inteligencji, takich jak rozpoznawanie wzorców, rozumienie języka naturalnego, podejmowanie decyzji, uczenie się i planowanie. Głównym celem AI jest stworzenie systemów, które są zdolne do myślenia i podejmowania decyzji na sposób przypominający ludzki. W potocznym rozumieniu, sztuczna inteligencja to coś (program, maszyna) symulujące inteligencję naturalną, ludzką.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Czym jest sztuczna inteligencja?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f851e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W tekście nie ma informacji na temat Gemini. Tekst opisuje Sztuczną Inteligencję Ogólnego Przeznaczenia (AGI).\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Co to jest Gemini?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a0e31cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Historia sztucznej inteligencji sięga lat 50. XX wieku, kiedy to powstały pierwsze koncepcje i modele tego, co mogłoby się nią stać. Jednym z pionierów był Alan Turing, który sformułował test Turinga, mający na celu ocenę zdolności maszyny do inteligentnego zachowania na poziomie ludzkim.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Opowiedz mi o poczatkach sztucznej inteligencji\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
